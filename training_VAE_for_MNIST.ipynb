{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders\n",
    "\n",
    "This architecture is very important to us, because autoencoders are a base for some popular generative models. Autoencoders have power to compress data and reconstruct data. Basicaly, this is very interesting because, first application can be compress data and you can spend less memory for any application (yay!). But autoencoder can be much more! Let's check this out and see architecture below:\n",
    "\n",
    "<!-- ![Example Image](./imgs/autoencoder_architecture.png) -->\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"./images/autoencoder_architecture.png\" alt=\"Example Image\" style=\"width:600px;\">\n",
    "</div>\n",
    "\n",
    "In here, we can see architecture and loss function that we must to use. This loss is called by \"Reconstruction Loss\", because when system minimizes this, we can to turn output aproximates by input. \n",
    "\n",
    "$$\\text{Reconstruction Loss} = \\text{MSE}(x, \\hat x)$$\n",
    "\n",
    "Before we in fact construct some generative architecture, lets build an autoencoder and check some theory subjects! Spoiler: You will learn about **latent space** (Latent space is a little bit abstact, think about him like a something in Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
